{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple conv. net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200/1000, Loss: 0.00047495323815383017\n",
      "Epoch: 400/1000, Loss: 0.000125280290376395\n",
      "Epoch: 600/1000, Loss: 5.607167986454442e-05\n",
      "Epoch: 800/1000, Loss: 3.141085835522972e-05\n",
      "Epoch: 1000/1000, Loss: 1.9860137399518862e-05\n",
      "String: the dog chased the cat, Contains 'cat': Yes\n",
      "String: cats are cute, Contains 'cat': Yes\n",
      "String: the cat is on the mat, Contains 'cat': Yes\n",
      "String: a rabbit ran past, Contains 'cat': No\n",
      "String: ccaatt, Contains 'cat': No\n"
     ]
    }
   ],
   "source": [
    "# Created by ChatGPT (OpenAI) - 2023\n",
    "# with manual edits\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the dataset\n",
    "random_strings = [\n",
    "    \"dog is playing with a ball\",\n",
    "    \"the cat is sleeping\",\n",
    "    \"the cats are sleeping\",\n",
    "    \"the cat jumped over the fence\",\n",
    "    \"birds are chirping in the trees\",\n",
    "    \"the cat is hiding under the bed\",\n",
    "    \"ccaccctat\",\n",
    "    \"ccaccctccaatt\",\n",
    "    \"ccaccctcatccatt\",\n",
    "]\n",
    "labels = [0, 1, 1, 1, 0, 1, 0, 0, 1]  # 1 if the string contains \"cat\", 0 otherwise\n",
    "\n",
    "# Define the vocabulary\n",
    "vocabulary = {'<pad>': 0, ' ': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7,\n",
    "              'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15,\n",
    "              'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23,\n",
    "              'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
    "\n",
    "# Convert strings to tensors\n",
    "# max_length = max(len(s) for s in random_strings)\n",
    "INPUT_LENGTH = 64\n",
    "X = torch.zeros((len(random_strings), INPUT_LENGTH), dtype=torch.long)\n",
    "for i, string in enumerate(random_strings):\n",
    "    for j, char in enumerate(string):\n",
    "        X[i, j] = vocabulary.get(char.lower(), 0)\n",
    "        \n",
    "# Define the ConvNet architecture\n",
    "class CatConvNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, kernel_sizes, hidden_dim):\n",
    "        super(CatConvNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embedding_dim, num_filters, kernel_size) for kernel_size in kernel_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2)  # Convert to shape: (batch_size, embedding_dim, sequence_length)\n",
    "        x = [torch.relu(conv(x)) for conv in self.convs]  # Apply convolutional layers\n",
    "        x = [torch.max_pool1d(conv_out, conv_out.size(2)).squeeze(2) for conv_out in x]  # Max pooling\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = torch.relu(self.fc(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "VOCAB_SIZE = len(vocabulary)\n",
    "embedding_dim = 16\n",
    "N_FILTERS = 32\n",
    "kernel_sizes = [3, 4, 5]\n",
    "hidden_dim = 64\n",
    "model = CatConvNet(VOCAB_SIZE, embedding_dim, N_FILTERS, kernel_sizes, hidden_dim)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward\n",
    "    output = model(X)\n",
    "    # print(output.shape, labels)\n",
    "    loss = criterion(output, torch.tensor(labels, dtype=torch.float32).unsqueeze(1))\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss at every 100 epochs\n",
    "    if (epoch + 1) % (num_epochs/5) == 0:\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Test the model\n",
    "test_strings = [\n",
    "    \"the dog chased the cat\",\n",
    "    \"cats are cute\",\n",
    "    \"the cat is on the mat\",\n",
    "    \"a rabbit ran past\",\n",
    "    \"ccaatt\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for string in test_strings:\n",
    "        test_X = torch.zeros((1, INPUT_LENGTH), dtype=torch.long)\n",
    "        for j, char in enumerate(string):\n",
    "            test_X[0, j] = vocabulary.get(char.lower(), 0)\n",
    "        \n",
    "        predicted = model(test_X)\n",
    "        predicted_label = int(predicted.round().item())\n",
    "        print(f\"String: {string}, Contains 'cat': {'Yes' if predicted_label == 1 else 'No'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stepping though model.forward() manually to figure out what is going on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 16,  8,  1, 10, 20,  1, 17, 13,  2, 26, 10, 15,  8,  1, 24, 10, 21,\n",
       "          9,  1,  2,  1,  3,  2, 13, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [21,  9,  6,  1,  4,  2, 21,  1, 10, 20,  1, 20, 13,  6,  6, 17, 10, 15,\n",
       "          8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_x = X[:2]; _x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3067, -0.0899,  1.2317,  ...,  0.1927, -0.5810, -0.6559],\n",
       "         [-0.1324, -2.3706, -1.6330,  ...,  1.5376,  0.0507, -0.3009],\n",
       "         [-0.5223,  0.9952, -0.8134,  ..., -0.3842, -0.7712,  1.1321],\n",
       "         ...,\n",
       "         [ 0.7829, -0.0130,  0.6944,  ...,  0.0903, -0.5905, -0.0296],\n",
       "         [ 0.7829, -0.0130,  0.6944,  ...,  0.0903, -0.5905, -0.0296],\n",
       "         [ 0.7829, -0.0130,  0.6944,  ...,  0.0903, -0.5905, -0.0296]],\n",
       "\n",
       "        [[ 1.4782, -1.5622, -1.5464,  ..., -0.6063, -0.8329,  2.3900],\n",
       "         [ 0.2707, -0.5611,  0.5882,  ..., -0.8165,  1.0123,  0.9820],\n",
       "         [-1.2866,  1.9623, -0.0764,  ...,  1.0592, -0.2838,  0.8894],\n",
       "         ...,\n",
       "         [ 0.7829, -0.0130,  0.6944,  ...,  0.0903, -0.5905, -0.0296],\n",
       "         [ 0.7829, -0.0130,  0.6944,  ...,  0.0903, -0.5905, -0.0296],\n",
       "         [ 0.7829, -0.0130,  0.6944,  ...,  0.0903, -0.5905, -0.0296]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_emb = model.embedding(_x)\n",
    "print(_emb.shape)\n",
    "_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3067, -0.1324, -0.5223,  ...,  0.7829,  0.7829,  0.7829],\n",
       "         [-0.0899, -2.3706,  0.9952,  ..., -0.0130, -0.0130, -0.0130],\n",
       "         [ 1.2317, -1.6330, -0.8134,  ...,  0.6944,  0.6944,  0.6944],\n",
       "         ...,\n",
       "         [ 0.1927,  1.5376, -0.3842,  ...,  0.0903,  0.0903,  0.0903],\n",
       "         [-0.5810,  0.0507, -0.7712,  ..., -0.5905, -0.5905, -0.5905],\n",
       "         [-0.6559, -0.3009,  1.1321,  ..., -0.0296, -0.0296, -0.0296]],\n",
       "\n",
       "        [[ 1.4782,  0.2707, -1.2866,  ...,  0.7829,  0.7829,  0.7829],\n",
       "         [-1.5622, -0.5611,  1.9623,  ..., -0.0130, -0.0130, -0.0130],\n",
       "         [-1.5464,  0.5882, -0.0764,  ...,  0.6944,  0.6944,  0.6944],\n",
       "         ...,\n",
       "         [-0.6063, -0.8165,  1.0592,  ...,  0.0903,  0.0903,  0.0903],\n",
       "         [-0.8329,  1.0123, -0.2838,  ..., -0.5905, -0.5905, -0.5905],\n",
       "         [ 2.3900,  0.9820,  0.8894,  ..., -0.0296, -0.0296, -0.0296]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_xtr = _emb.transpose(1, 2)\n",
    "print(_xtr.shape)\n",
    "_xtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([2, 32, 62]), torch.Size([2, 32, 61]), torch.Size([2, 32, 60])]\n",
      "[62, 61, 60]\n"
     ]
    }
   ],
   "source": [
    "_xconv = [torch.relu(conv(_xtr)) for conv in model.convs]\n",
    "print([c.shape for c in _xconv])\n",
    "print([c.size(2) for c in _xconv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([2, 32]), torch.Size([2, 32]), torch.Size([2, 32])]\n"
     ]
    }
   ],
   "source": [
    "_xpool = [torch.max_pool1d(conv_out, conv_out.size(2)).squeeze(2) for conv_out in _xconv]  # Max pooling\n",
    "print([p.shape for p in _xpool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 96])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_xcat = torch.cat(_xpool, dim=1)\n",
    "_xcat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_xfc = torch.relu(model.fc(_xcat))\n",
    "_xfc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_xo = torch.sigmoid(model.output(_xfc))\n",
    "_xo.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plan procedure with one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 32])\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "max_len, VOCAB_SIZE = 64, 32\n",
    "\n",
    "_xoh = nn.functional.one_hot(_x, num_classes=VOCAB_SIZE)\n",
    "print(_xoh.shape)\n",
    "print([int(x) for x in _xoh[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_xtr = _xoh.transpose(1, 2)  # Convert to shape: (batch_size, embedding_dim, sequence_length)\n",
    "_xtr = _xtr.float()\n",
    "_xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 60])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FITLERS = 8\n",
    "KS = 5\n",
    "STRIDE = 1\n",
    "f_conv = nn.Conv1d(VOCAB_SIZE, FITLERS, KS, STRIDE)\n",
    "_xconv = f_conv(_xtr)\n",
    "_xconv.shape # samples, filters, signal at each conv. step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _signal_len(l, stride, kernel):\n",
    "    return (l // stride) - kernel + 1\n",
    "_conv_signal_len = _signal_len(max_len, STRIDE, KS)\n",
    "print(_conv_signal_len)\n",
    "\n",
    "f_pool = nn.MaxPool1d(_conv_signal_len) # pool all conv. steps in one go\n",
    "_xpool = f_pool(_xconv)\n",
    "_xpool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([p.squeeze(2) for p in [_xpool, _xpool]], dim=1).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make model with one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "VOCAB_SIZE = 32\n",
    "vocabulary = {'░': 0, ' ': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7,\n",
    "              'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15,\n",
    "              'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23,\n",
    "              'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
    "\n",
    "inv_vocab = {v:k for k, v in vocabulary.items()}\n",
    "\n",
    "def _make_train_set(min_len, max_len, n, target: str):\n",
    "    sd = (max_len - min_len)/4\n",
    "    av_len = (max_len+min_len)/2\n",
    "    lengths = np.random.normal(av_len, sd, n).round().clip(min_len, max_len).astype(int)\n",
    "    true_cases = np.random.random(n) > 0.5\n",
    "    entries = []\n",
    "    for l, is_t in zip(lengths, true_cases):\n",
    "        entry = [inv_vocab[k] for k in np.random.randint(1, len(vocabulary), size=l)]\n",
    "        if is_t:\n",
    "            pos = np.random.randint(0, l-len(target))\n",
    "            for i, c in enumerate(target):\n",
    "                entry[pos+i] = c\n",
    "        entries.append((\"\".join(entry), is_t))\n",
    "\n",
    "    return entries\n",
    "\n",
    "\n",
    "INPUT_LENGTH = 64\n",
    "N_EXAMPLES = 1000 \n",
    "TARGET = \"cat\"\n",
    "train_cases = _make_train_set(INPUT_LENGTH//2, INPUT_LENGTH, N_EXAMPLES, TARGET)\n",
    "train_x = torch.zeros((len(train_cases), INPUT_LENGTH), dtype=torch.long)\n",
    "train_y = [ans for case, ans in train_cases]\n",
    "for i, (string, ans) in enumerate(train_cases):\n",
    "    for j, char in enumerate(string):\n",
    "        train_x[i, j] = vocabulary.get(char.lower(), 0)\n",
    "\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0200 of 1000, Loss: 0.26836\n",
      "Epoch: 0400 of 1000, Loss: 0.07356\n",
      "Epoch: 0600 of 1000, Loss: 0.03105\n",
      "Epoch: 0800 of 1000, Loss: 0.01647\n",
      "Epoch: 1000 of 1000, Loss: 0.00999\n",
      "String: the dog chased the cat, Contains 'cat': Yes\n",
      "String: cats are cute, Contains 'cat': Yes\n",
      "String: the cat is on the mat, Contains 'cat': Yes\n",
      "String: a rabbit ran past, Contains 'cat': No\n",
      "String: ccaatt, Contains 'cat': Yes\n"
     ]
    }
   ],
   "source": [
    "class ConvOH(nn.Module):\n",
    "    def __init__(self, kernels) -> None:\n",
    "        super(ConvOH, self).__init__()\n",
    "        def _signal_len(l, kernel, stride=1):\n",
    "            return (l // stride) - kernel + 1\n",
    "\n",
    "        def _one_hot(x): # deterministic, since x is indicies\n",
    "            _oh = nn.functional.one_hot(x, num_classes=VOCAB_SIZE)\n",
    "            # swap axis 1 and 2\n",
    "            # tokens X OH embedding -> OH embedding channel X token is this category (channel)\n",
    "            # ex. scanning through channel for char \"k\", a 1 == this token is \"k\", else not \"k\"\n",
    "            return _oh.transpose(1, 2).float()\n",
    "        \n",
    "        self.l_conv_kernels = nn.ModuleList([\n",
    "            nn.Conv1d(VOCAB_SIZE, n, s)\n",
    "        for n, s in kernels])\n",
    "        def _conv(x):\n",
    "            return [torch.relu(c(x)) for c in self.l_conv_kernels]\n",
    "        \n",
    "        self.l_pools = nn.ModuleList([\n",
    "            nn.MaxPool1d(_signal_len(INPUT_LENGTH, s))\n",
    "        for n, s in kernels])\n",
    "        def _pool(x):\n",
    "            # pool hits for each kernel\n",
    "            # squeeze (remove) the last axis since pooling entire signal length so len(axis2)==1\n",
    "            # hits is [samples X filters for k in kernels]\n",
    "            hits = [p(v).squeeze(2) for p, v in zip(self.l_pools, x)]\n",
    "            return torch.cat(hits, dim=1) # concat hits for each kernel\n",
    "\n",
    "        self.l_out = nn.Linear(sum([n for n, s in kernels]), 1)\n",
    "        def _infer(x):\n",
    "            # examine results across all filters to find hits\n",
    "            return torch.sigmoid(self.l_out(x))\n",
    "        \n",
    "        self._procedure = [\n",
    "            _one_hot, _conv, _pool, _infer\n",
    "        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        _temp = x\n",
    "        for fn in self._procedure:\n",
    "            _temp = fn(_temp)\n",
    "        return _temp\n",
    "\n",
    "    \n",
    "model_oh = ConvOH([ # (filters, size)\n",
    "    (12, 5),\n",
    "])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_oh.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model_oh.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward\n",
    "    output = model_oh(train_x)\n",
    "    # print(output.shape, labels)\n",
    "    loss = criterion(output, torch.tensor(train_y, dtype=torch.float32).unsqueeze(1))\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss at every 100 epochs\n",
    "    if (epoch + 1) % (num_epochs/5) == 0:\n",
    "        print(f\"Epoch: {epoch+1:04} of {num_epochs}, Loss: {loss.item():0.5f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Test the model\n",
    "test_strings = [\n",
    "    \"the dog chased the cat\",\n",
    "    \"cats are cute\",\n",
    "    \"the cat is on the mat\",\n",
    "    \"a rabbit ran past\",\n",
    "    \"ccaatt\",\n",
    "]\n",
    "\n",
    "model_oh.eval()\n",
    "with torch.no_grad():\n",
    "    for string in test_strings:\n",
    "        test_X = torch.zeros((1, INPUT_LENGTH), dtype=torch.long)\n",
    "        for j, char in enumerate(string):\n",
    "            test_X[0, j] = vocabulary.get(char.lower(), 0)\n",
    "        \n",
    "        predicted = model_oh(test_X)\n",
    "        predicted_label = int(predicted.round().item())\n",
    "        print(f\"String: {string}, Contains 'cat': {'Yes' if predicted_label == 1 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catuz [0.6710437  0.6439772  0.5972075  0.35859096 0.34459642]\n",
      "jcpaz [0.57144797 0.5423914  0.62578523 0.73969066 0.5217017 ]\n",
      "gcatr [0.23899287 0.5146104  0.5090941  0.46041834 0.35250074]\n",
      "p cat [0.2855157  0.2708299  0.49754673 0.4118676  0.5134269 ]\n",
      "tacat [0.45216578 0.55285364 0.50897306 0.5009853  0.57777077]\n",
      "samsf [0.7051544  0.6818988  0.7662786  0.66386276 0.6871013 ]\n",
      "ifgjr [0.6068009  0.55019397 0.55477536 0.5676706  0.6157469 ]\n",
      "pheos [0.65311545 0.6532108  0.7731761  0.7372977  0.60265785]\n",
      "catez [0.7484057  0.77040267 0.66571456 0.28688735 0.30866337]\n",
      "wnclj [0.6326445  0.5397061  0.68225783 0.67811763 0.63919127]\n",
      "attij [0.73628515 0.63500434 0.21461163 0.22101949 0.19894506]\n",
      "acatx [0.2685592  0.5878102  0.52868205 0.59639597 0.26977998]\n"
     ]
    }
   ],
   "source": [
    "def decode(filter):\n",
    "    f: np.ndarray = filter.transpose(0, 1).detach().numpy()\n",
    "    result = []\n",
    "    for emb in f:\n",
    "        i = np.argmax(emb)\n",
    "        result.append(inv_vocab[i])\n",
    "    return \"\".join(result), f.max(axis=1)\n",
    "\n",
    "for kernel in model_oh.l_conv_kernels:\n",
    "    for f in kernel.weight:\n",
    "        s, scores = decode(f)\n",
    "        print(s, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.6629, -0.8011,  0.7902,  0.8332,  0.6761, -0.8127, -0.8117, -0.7340,\n",
       "          0.7401, -0.8250,  0.8709,  0.7557]], requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_oh.l_out.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
